# -*- coding: utf-8 -*-
"""final_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14jTeevB2tUpet4R4ntAh6h2ZTNmkhaWd
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge
from sklearn.linear_model import TweedieRegressor
from sklearn.linear_model import Lasso
from sklearn.cross_decomposition import PLSRegression
from sklearn.neural_network import MLPRegressor
from sklearn.linear_model import SGDRegressor
from sklearn.tree import DecisionTreeRegressor 
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.ensemble import AdaBoostRegressor
from sklearn.ensemble import GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.linear_model import Ridge
from sklearn.model_selection import RandomizedSearchCV
from sklearn.linear_model import Lasso
from sklearn.linear_model import ElasticNet

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV

from sklearn.metrics import r2_score
from sklearn.model_selection import cross_val_score
from sklearn import metrics
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
# %matplotlib inline

from google.colab import drive
drive.mount('/content/drive')

"""## Upload dataset"""

#C:\\Users\\ASUS\\Desktop\\shai-ml\\
train_data = pd.read_csv('/content/drive/MyDrive/shai-ml/train.csv')

test_data = pd.read_csv('/content/drive/MyDrive/shai-ml/test.csv')

#split diamond data into data & labels
from sklearn.model_selection import train_test_split
train_set ,test_set = train_test_split(train_data,test_size = 0.2,random_state = 42 )

train_set.head()

test_set.head()

train_set.info()

# Drop the "Unnamed: 0" column
train_set = train_set.drop("Unnamed: 0", axis = 1)

# Price is int64, best if all numeric attributes have the same datatype, especially as float64
train_set["price"] = train_set["price"].astype(float)

# Preview dataset again
train_set.head()

#Checking for duplicate records in the data
dups = train_set .duplicated()
print('Number of duplicate rows = %d' % (dups.sum()))
print(train_set .shape)

print('Before',train_set.shape)
train_set.drop_duplicates(inplace=True) 
print('After',train_set.shape)

train_set["cut"].value_counts()

train_set["color"].value_counts()

train_set["clarity"].value_counts()

train_set.describe()

"""## Data Analysis & Visualization"""

corr_matrix = train_set.corr()
corr_matrix["price"].sort_values(ascending=False)

train_set.hist(bins = 40, figsize = (20,15))
plt.show()

ans =sns.pairplot(train_set)

# Plot the correlation with seaborn
plt.subplots(figsize = (10, 8))
sns.heatmap(corr_matrix, annot = True)
plt.show()

sns.distplot(train_set.price)

train_set.isna().sum()

print("Number of rows with x == 0: {} ".format((train_set.x==0).sum()))
print("Number of rows with y == 0: {} ".format((train_set.y==0).sum()))
print("Number of rows with z == 0: {} ".format((train_set.z==0).sum()))
print("Number of rows with depth == 0: {} ".format((train_set.depth==0).sum()))

train_set[['x','y','z']] = train_set[['x','y','z']].replace(0,np.NaN)

train_set.isnull().sum()

train_set.dropna(inplace=True)

train_set.isnull().sum()

train_set.shape

ans = sns.countplot(data = train_set,x = 'cut')

ans = sns.barplot(data = train_set, x ='cut', y='price')

ans = sns.countplot(data = train_set,x = 'color')

ans = sns.barplot(data = train_set, x ='color', y='price')

ans = sns.countplot(data = train_set,x = 'clarity')

ans = sns.barplot(data = train_set, x ='clarity', y='price')

layout_options = {
    'paper_bgcolor':"#383838",
    'plot_bgcolor':'#383838',
    'title_font': dict(color='white'),
    'legend_font': dict(color='white'),
    'yaxis':dict(color="white"),
    'xaxis':dict(color="white")
    }

"""## Feature Engineering"""

train_set['space'] = train_set['x'] * train_set['y']
train_set['volume'] = train_set['x'] * train_set['y'] * train_set['z']

corr_matrix = train_set.corr()
corr_matrix["price"].sort_values(ascending=False)

train_set.drop(['x','y','z'], axis = 1, inplace = True)
train_set.head()

"""##One hot Encoding"""

one_hot_encoders_train_data =  pd.get_dummies(train_set)
cols = one_hot_encoders_train_data.columns
train_clean_data = pd.DataFrame(one_hot_encoders_train_data,columns= cols)
train_clean_data.head()

from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
numericals =  pd.DataFrame(sc_X.fit_transform(train_clean_data[['carat','depth','table','space','volume']]),
                           columns=['carat','depth','table','space','volume'],
                           index=train_clean_data.index)


train_clean_data_standard = train_clean_data.copy(deep=True)
train_clean_data_standard[['carat','depth','table','space','volume']] = numericals[['carat','depth','table','space','volume']]
train_clean_data_standard.head()

X_train = train_clean_data_standard.drop(["price"],axis=1)
Y_train = train_clean_data_standard.price

X_train.shape

"""### Preprocessing Test data"""

# Drop the "Unnamed: 0" column
test_set = test_set.drop("Unnamed: 0", axis = 1)

test_set['space'] = test_set['x'] * test_set['y']
test_set['volume'] = test_set['x'] * test_set['y'] * test_set['z']
test_set.drop(['x','y','z'], axis = 1, inplace = True)

one_hot_encoders_test_data =  pd.get_dummies(test_set)
cols = one_hot_encoders_test_data.columns
test_clean_data = pd.DataFrame(one_hot_encoders_test_data,columns= cols)

numericals =  pd.DataFrame(sc_X.transform(test_clean_data[['carat','depth','table','space','volume']]),
                           columns=['carat','depth','table','space','volume'],
                           index=test_clean_data.index)

test_clean_data_standard = test_clean_data.copy(deep=True)
test_clean_data_standard[['carat','depth','table','space','volume']] = numericals[['carat','depth','table','space','volume']]
X_test = test_clean_data_standard.drop(["price"],axis=1)
X_test.head()

X_test.shape

Y_test = test_set.price

"""### Select and Train Model"""

#CV = []
#R2_train = []
rmse_ = []
def evaluation_train(model):

  model.fit(X_train,Y_train)
  Y_pred = model.predict(X_train)
  # MAE
  mae =mean_absolute_error(Y_train, Y_pred)
    
  # MSE
  lin_mse = mean_squared_error(Y_train, Y_pred)
  lin_rmse = np.sqrt(lin_mse)


  scores = cross_val_score(model, X_train, Y_train, scoring ="neg_mean_squared_error",cv = 10)
  rmse_scores = np.sqrt(-scores)

  return mae,lin_rmse,rmse_scores,rmse_scores.mean(),rmse_scores.std()

def meansquarederror(y_pred, y_test):
    lin_mse = mean_squared_error(y_pred, y_test)
    lin_rmse = np.sqrt(lin_mse)
    return lin_rmse

"""#### Linear Regression Model"""

X_train.shape

model_1 = LinearRegression()
model_1.fit(X_train,Y_train)
mse,lin_rmse,rmse_scores,rmse_scores_mean,rmse_scores_std =evaluation_train(model_1)
print("MAE: ",lin_rmse)
print("MSE: ",mse)
print("Scores: ", rmse_scores)
print("Mean: ", rmse_scores_mean)
print("Standard Deviation: ", rmse_scores_std)

y_pred=model_1.predict(X_test)
rmse_.append(meansquarederror(y_pred, Y_test))
meansquarederror(y_pred, Y_test)

"""#### Decision Tree Regressor Model"""

model_2 = DecisionTreeRegressor()
model_2.fit(X_train,Y_train)

mse,lin_rmse,rmse_scores,rmse_scores_mean,rmse_scores_std =evaluation_train(model_2)
print("MAE: ",lin_rmse)
print("MSE: ",mse)
print("Scores: ", rmse_scores)
print("Mean: ", rmse_scores_mean)
print("Standard Deviation: ", rmse_scores_std)

y_pred=model_2.predict(X_test)
rmse_.append(meansquarederror(y_pred, Y_test))
meansquarederror(y_pred, Y_test)

"""####  Random Forest Regressor Model"""

model_3 = RandomForestRegressor()
model_3.fit(X_train,Y_train)

mse,lin_rmse,rmse_scores,rmse_scores_mean,rmse_scores_std =evaluation_train(model_3)
print("MAE: ",lin_rmse)
print("MSE: ",mse)
print("Scores: ", rmse_scores)
print("Mean: ", rmse_scores_mean)
print("Standard Deviation: ", rmse_scores_std)

y_pred=model_3.predict(X_test)
rmse_.append(meansquarederror(y_pred, Y_test))
meansquarederror(y_pred, Y_test)

"""####Support Vector Machines"""

rbf = SVR(kernel="rbf")
rbf.fit(X_train,Y_train)

mse,lin_rmse,rmse_scores,rmse_scores_mean,rmse_scores_std =evaluation_train(rbf)
print("MAE: ",lin_rmse)
print("MSE: ",mse)
print("Scores: ", rmse_scores)
print("Mean: ", rmse_scores_mean)
print("Standard Deviation: ", rmse_scores_std)

y_pred=rbf.predict(X_test)
rmse_.append(meansquarederror(y_pred, Y_test))
meansquarederror(y_pred, Y_test)

linear = SVR(kernel="linear")
linear.fit(X_train,Y_train)

mse,lin_rmse,rmse_scores,rmse_scores_mean,rmse_scores_std =evaluation_train(linear)
print("MAE: ",lin_rmse)
print("MSE: ",mse)
print("Scores: ", rmse_scores)
print("Mean: ", rmse_scores_mean)
print("Standard Deviation: ", rmse_scores_std)

y_pred=linear.predict(X_test)
rmse_.append(meansquarederror(y_pred, Y_test))
meansquarederror(y_pred, Y_test)

"""#### Random Forest Regressor Hyperparameter tuning"""

best_model = RandomForestRegressor(n_estimators=400,
                                  max_features='sqrt',
                                  )

mse,lin_rmse,rmse_scores,rmse_scores_mean,rmse_scores_std =evaluation_train(best_model)
print("MAE: ",lin_rmse)
print("MSE: ",mse)
print("Scores: ", rmse_scores)
print("Mean: ", rmse_scores_mean)
print("Standard Deviation: ", rmse_scores_std)

y_pred=best_model.predict(X_test)
rmse_.append(meansquarederror(y_pred, Y_test))
meansquarederror(y_pred, Y_test)

model_7 = RandomForestRegressor(criterion= 'squared_error',n_estimators= 500)
model_7.fit(X_train,Y_train)

mse,r_rmse,rmse_scores,rmse_scores_mean,rmse_scores_std =evaluation_train(model_7)
print("MAE: ",r_rmse)
print("MSE: ",mse)
print("Scores: ", rmse_scores)
print("Mean: ", rmse_scores_mean)
print("Standard Deviation: ", rmse_scores_std)

y_pred=model_7.predict(X_test)
rmse_.append(meansquarederror(y_pred, Y_test))
meansquarederror(y_pred, Y_test)

"""#### Grid Search"""

parameters = {
   'nthread':[x for x in range(1,6)], 
   'objective':['reg:squarederror'],
   'learning_rate': [.01,.03, 0.05,0.02], 
    'max_depth': [x for x in range(4,10)],
     'n_estimators': [500,700,200,400,800]
}

RegModel=XGBRegressor()

grid_search = GridSearchCV(RegModel, parameters, cv=5,scoring='neg_mean_squared_error',return_train_score=True)

grid_search.fit(X_train,Y_train)
y_pred=grid_search.predict(X_test)
meansquarederror(y_pred, y_test)

"""#### AdaBoost Regression"""

model_6 = AdaBoostRegressor(n_estimators = 100)
model_6.fit(X_train,Y_train)

mse,lin_rmse,rmse_scores,rmse_scores_mean,rmse_scores_std =evaluation_train(model_6)
print("MAE: ",lin_rmse)
print("MSE: ",mse)
print("Scores: ", rmse_scores)
print("Mean: ", rmse_scores_mean)
print("Standard Deviation: ", rmse_scores_std)

y_pred=model_6.predict(X_test)
rmse_.append(meansquarederror(y_pred, Y_test))
meansquarederror(y_pred, Y_test)

"""#### GradientBoosting Regression"""

model_7 = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,max_depth=1)
model_7.fit(X_train,Y_train)

mse,lin_rmse,rmse_scores,rmse_scores_mean,rmse_scores_std =evaluation_train(model_7)
print("MAE: ",lin_rmse)
print("MSE: ",mse)
print("Scores: ", rmse_scores)
print("Mean: ", rmse_scores_mean)
print("Standard Deviation: ", rmse_scores_std)

y_pred=model_7.predict(X_test)
rmse_.append(meansquarederror(y_pred, Y_test))
meansquarederror(y_pred, Y_test)

"""####XGBRegressor"""

model_8 = XGBRegressor()
model_8.fit(X_train,Y_train)

mse,lin_rmse,rmse_scores,rmse_scores_mean,rmse_scores_std =evaluation_train(model_8)
print("MAE: ",lin_rmse)
print("MSE: ",mse)
print("Scores: ", rmse_scores)
print("Mean: ", rmse_scores_mean)
print("Standard Deviation: ", rmse_scores_std)

y_pred=model_8.predict(X_test)
rmse_.append(meansquarederror(y_pred, Y_test))
meansquarederror(y_pred, Y_test)

model_9=XGBRegressor(colsample_bytree= 0.7,learning_rate= 0.03,max_depth= 7,min_child_weight= 5,n_estimators= 500,nthread= 1,objective= 'reg:squarederror',subsample= 0.7)
model_9.fit(X_train,Y_train)

mse,lin_rmse,rmse_scores,rmse_scores_mean,rmse_scores_std =evaluation_train(model_9)
print("MAE: ",lin_rmse)
print("MSE: ",mse)
print("Scores: ", rmse_scores)
print("Mean: ", rmse_scores_mean)
print("Standard Deviation: ", rmse_scores_std)

y_pred=model_9.predict(X_test)
rmse_.append(meansquarederror(y_pred, Y_test))
meansquarederror(y_pred, Y_test)

model_10=XGBRegressor(colsample_bytree= 0.7,learning_rate= 0.01,max_depth= 7,min_child_weight= 5,n_estimators= 500,nthread= 1,objective= 'reg:squarederror',subsample= 0.7)
model_10.fit(X_train,Y_train)

mse,lin_rmse,rmse_scores,rmse_scores_mean,rmse_scores_std =evaluation_train(model_10)
print("MAE: ",lin_rmse)
print("MSE: ",mse)
print("Scores: ", rmse_scores)
print("Mean: ", rmse_scores_mean)
print("Standard Deviation: ", rmse_scores_std)

y_pred=model_10.predict(X_test)
rmse_.append(meansquarederror(y_pred, Y_test))
meansquarederror(y_pred, Y_test)

model_11=XGBRegressor(learning_rate=0.05, max_depth=7, n_estimators=500, nthread=1,
             objective='reg:squarederror')
model_11.fit(X_train,Y_train)

mse,lin_rmse,rmse_scores,rmse_scores_mean,rmse_scores_std =evaluation_train(model_11)
print("MAE: ",lin_rmse)
print("MSE: ",mse)
print("Scores: ", rmse_scores)
print("Mean: ", rmse_scores_mean)
print("Standard Deviation: ", rmse_scores_std)

y_pred=model_11.predict(X_test)
rmse_.append(meansquarederror(y_pred, Y_test))
meansquarederror(y_pred, Y_test)

